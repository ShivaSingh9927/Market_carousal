{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d811e1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1564896/1103047228.py:24: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent_executor = create_react_agent(model, tools, checkpointer=memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è‚Äç‚ôÇÔ∏è Scout Agent is searching for technical truths...\n",
      "\n",
      "--- Scout Progress ---\n",
      "[{\"snippet\": \"12 Jan 2026 ¬∑ Key capabilities. Direct local file access: Claude can read from and write to your local files without manual uploads or downloads. Sub-agent coordination ...\", \"title\": \"Getting started with Cowork | Claude Help Center\", \"link\": \"https://support.claude.com/en/articles/13345190-getting-started-with-cowork\"}, {\"snippet\": \"16 Jan 2026 ¬∑ It runs in a terminal and gives Claude the ability to read codebases, write files, and execute commands. ... locally, or use local files as input for external ...\", \"title\": \"Claude Cowork Tutorial: How to Use Anthropic's AI Desktop Agent\", \"link\": \"https://www.datacamp.com/tutorial/claude-cowork-tutorial\"}, {\"snippet\": \"12 Jan 2026 ¬∑ Cowork brings Claude Code's agentic capabilities to the Claude desktop app. Give Claude access to a folder, set a task, and let it work. Missing: local filesystem\", \"title\": \"Introducing Cowork: Claude Code for the rest of your work - YouTube\", \"link\": \"https://www.youtube.com/watch?v=UAmKyyZ-b9E\"}, {\"snippet\": \"12 Jan 2026 ¬∑ The general verdict is that Cowork is basically Claude Code with a pretty UI for your non-tech coworkers who are scared of the command line. It's seen as a good ... Claude introduces Cowork: Claude Code for the rest of your work Anthropic just launched \\\"Claude Cowork\\\" for $100/mo. I built ... More results from www.reddit.com\", \"title\": \"Claude just introduced Cowork: the Claude code for non-dev stuff - Reddit\", \"link\": \"https://www.reddit.com/r/ClaudeAI/comments/1qb6gdx/claude_just_introduced_cowork_the_claude_code_for/\"}]\n",
      "\n",
      "--- Scout Progress ---\n",
      "Based on the search results, here are three specific things that Claude Coworker by Anthropic can do with a local filesystem that a standard chatbot cannot:\n",
      "\n",
      "1. Direct local file access: Claude can read from and write to local files without manual uploads or downloads.\n",
      "2. Sub-agent coordination: Claude can coordinate with other agents to perform tasks that require access to local files.\n",
      "3. Execute commands: Claude can execute commands on the local machine, allowing it to interact with the local filesystem in a more powerful way.\n",
      "\n",
      "As for the \"Marketing Gap\", one potential area where Claude Coworker may not handle well is privacy concerns. The ability to access and write to local files raises potential security and privacy risks, particularly if the user is not careful about granting access to sensitive files or folders. Additionally, the fact that Claude Coworker can execute commands on the local machine may also raise concerns about the potential for malicious activity. Therefore, Anthropic may need to address these concerns through clear documentation, user education, and robust security measures to mitigate these risks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# 1. Setup Environment and Memory\n",
    "load_dotenv()\n",
    "memory = MemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": \"scout_001\"}}\n",
    "\n",
    "# 2. Initialize Model (Llama 3.3 via Groq)\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\", max_tokens=4000)\n",
    "\n",
    "# 3. Define the Search Tool\n",
    "# We use DuckDuckGo to find latest competitor/product news\n",
    "web_search_tool = DuckDuckGoSearchResults(output_format=\"list\")\n",
    "tools = [web_search_tool]\n",
    "\n",
    "# 4. Create the React Agent (The Scout)\n",
    "# This agent can now \"Think\" and then \"Search\" if it lacks info\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "# 5. Execute the Scout Query\n",
    "# We ask it to find technical truths to avoid \"rubbish\" content\n",
    "scout_query = \"\"\"\n",
    "Search for the latest technical capabilities of 'Claude Coworker' by Anthropic. \n",
    "Identify 3 specific things it can do with a local filesystem that a standard chatbot cannot.\n",
    "Then, identify a 'Marketing Gap'‚Äîsomething it doesn't handle well (like privacy concerns or specific industries).\n",
    "\"\"\"\n",
    "\n",
    "print(\"üïµÔ∏è‚Äç‚ôÇÔ∏è Scout Agent is searching for technical truths...\")\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=scout_query)]}, config\n",
    "):\n",
    "    for value in event.values():\n",
    "        # This will print the steps (Thought, Action, Observation)\n",
    "        if \"messages\" in value:\n",
    "            last_msg = value[\"messages\"][-1]\n",
    "            if hasattr(last_msg, 'content') and last_msg.content:\n",
    "                print(f\"\\n--- Scout Progress ---\\n{last_msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56183e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è‚Äç‚ôÇÔ∏è Lightweight Scouting: https://www.anthropic.com/news/claude-3-5-sonnet...\n",
      "‚úÖ Intelligence Saved: /nuvodata/User_data/shiva/Market_carousal/intelligence/Mon_scout.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Announcements\\nClaude 3.5 Sonnet\\nJun 21, 2024\\nTry on Claude.ai\\nUpdate\\nConsumer Terms and Privacy Policy\\nAug 28, 2025\\nToday, we‚Äôre launching Claude 3.5 Sonnet‚Äîour first release in the forthcoming Claude 3.5 model family. Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet.\\nClaude 3.5 Sonnet is now available for free on Claude.ai and the Claude iOS app, while Claude Pro and Team plan subscribers can access it with significantly higher rate limits. It is also available via the Anthropic \\nAPI\\n, \\nAmazon Bedrock\\n, and \\nGoogle Cloud‚Äôs Vertex AI\\n. The model costs $3 per million input tokens and $15 per million output tokens, with a 200K token context window.\\nFrontier intelligence at 2x the speed\\nClaude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval). It shows marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone.\\nClaude 3.5 Sonnet operates at twice the speed of Claude 3 Opus. This performance boost, combined with cost-effective pricing, makes Claude 3.5 Sonnet ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows.\\nIn an \\ninternal agentic coding evaluation\\n, Claude 3.5 Sonnet solved 64% of problems, outperforming Claude 3 Opus which solved 38%. Our evaluation tests the model‚Äôs ability to fix a bug or add functionality to an open source codebase, given a natural language description of the desired improvement. When instructed and \\nprovided with the relevant tools\\n, Claude 3.5 Sonnet can independently write, edit, and execute code with sophisticated reasoning and troubleshooting capabilities. It handles code translations with ease, making it particularly effective for updating legacy applications and migrating codebases.\\nState-of-the-art vision\\nClaude 3.5 Sonnet is our strongest vision model yet, surpassing Claude 3 Opus on standard vision benchmarks. These step-change improvements are most noticeable for tasks that require visual reasoning, like interpreting charts and graphs. Claude 3.5 Sonnet can also accurately transcribe text from imperfect images‚Äîa core capability for retail, logistics, and financial services, where AI may glean more insights from an image, graphic or illustration than from text alone.\\nArtifacts‚Äîa new way to use Claude\\nToday, we‚Äôre also introducing Artifacts on Claude.ai, a new feature that expands how users can interact with Claude. When a user asks Claude to generate content like code snippets, text documents, or website designs, these Artifacts appear in a dedicated window alongside their conversation. This creates a dynamic workspace where they can see, edit, and build upon Claude‚Äôs creations in real-time, seamlessly integrating AI-generated content into their projects and workflows.\\nThis preview feature marks Claude‚Äôs evolution from a conversational AI to a collaborative work environment. It‚Äôs just the beginning of a broader vision for Claude.ai, which will soon expand to support team collaboration. In the near future, teams‚Äîand eventually entire organizations‚Äîwill be able to securely centralize their knowledge, documents, and ongoing work in one shared space, with Claude serving as an on-demand teammate.\\nCommitment to safety and privacy\\nOur models are subjected to rigorous testing and have been trained to reduce misuse. Despite Claude 3.5 Sonnet‚Äôs leap in intelligence, our red teaming assessments have concluded that Claude 3.5 Sonnet remains at \\nASL-2\\n. More details can be found in the \\nmodel card addendum\\n.\\nAs part of our commitment to safety and transparency, we‚Äôve engaged with external experts to test and refine the safety mechanisms within this latest model. We recently provided Claude 3.5 Sonnet to the UK‚Äôs Artificial Intelligence Safety Institute (UK AISI) for pre-deployment safety evaluation. The UK AISI completed tests of 3.5 Sonnet and shared their results with the US AI Safety Institute (US AISI) as part of a Memorandum of Understanding, made possible by the partnership between the US and UK AISIs \\nannounced earlier this year\\n.\\nWe have integrated policy feedback from outside subject matter experts to ensure that our evaluations are robust and take into account new trends in abuse. This engagement has helped our teams scale up our ability to evaluate 3.5 Sonnet against various types of misuse. For example, we used feedback from child safety experts at \\nThorn\\n to update our classifiers and fine-tune our models.\\nOne of the core constitutional principles that guides our AI model development is privacy. We do not train our generative models on user-submitted data unless a user gives us explicit permission to do so.\\nComing soon\\nOur aim is to substantially improve the tradeoff curve between intelligence, speed, and cost every few months. To complete the Claude 3.5 model family, we‚Äôll be releasing Claude 3.5 Haiku and Claude 3.5 Opus later this year.\\nIn addition to working on our next-generation model family, we are developing new modalities and features to support more use cases for businesses, including integrations with enterprise applications. Our team is also exploring features like Memory, which will enable Claude to remember a user‚Äôs preferences and interaction history as specified, making their experience even more personalized and efficient.\\nWe‚Äôre constantly working to improve Claude and love hearing from our users. You can submit feedback on Claude 3.5 Sonnet directly in-product to inform our development roadmap and help our teams to improve your experience. As always, we look forward to seeing what you build, create, and discover with Claude.\\nRelated content\\nIntroducing Claude Opus 4.6\\nWe‚Äôre upgrading our smartest model. Across agentic coding, computer use, tool use, search, and finance, Opus 4.6 is an industry-leading model, often by wide margin. \\nRead more\\nClaude is a space to think\\nWe‚Äôve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust.\\nRead more\\nApple‚Äôs Xcode now supports the Claude Agent SDK\\nRead more\\nIntroducing Claude 3.5 Sonnet \\\\ Anthropic'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def light_scout(url, day_name=\"Mon\"):\n",
    "    print(f\"üïµÔ∏è‚Äç‚ôÇÔ∏è Lightweight Scouting: {url}...\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove \"Rubbish\" (Scripts, Styles, Nav, Footer)\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            element.decompose()\n",
    "            \n",
    "        # Get clean text\n",
    "        clean_text = soup.get_text(separator='\\n')\n",
    "        \n",
    "        # Save to your Intelligence folder\n",
    "        intel_path = f\"/nuvodata/User_data/shiva/Market_carousal/intelligence/{day_name}_scout.txt\"\n",
    "        os.makedirs(os.path.dirname(intel_path), exist_ok=True)\n",
    "        \n",
    "        with open(intel_path, \"w\") as f:\n",
    "            f.write(clean_text)\n",
    "            \n",
    "        print(f\"‚úÖ Intelligence Saved: {intel_path}\")\n",
    "        return clean_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Light Scout failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Usage:\n",
    "light_scout(\"https://www.anthropic.com/news/claude-3-5-sonnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca1e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Parallel Scout for 2 targets...\n",
      "‚úÖ Saved: /nuvodata/User_data/shiva/Market_carousal/intelligence/competitor_1.txt\n",
      "‚úÖ Saved: /nuvodata/User_data/shiva/Market_carousal/intelligence/competitor_2.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- CONFIG ---\n",
    "INTEL_DIR = \"/nuvodata/User_data/shiva/Market_carousal/intelligence\"\n",
    "os.makedirs(INTEL_DIR, exist_ok=True)\n",
    "\n",
    "# Optimized Session for connection reuse\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36\"\n",
    "})\n",
    "\n",
    "def optimized_fetch(url):\n",
    "    \"\"\"Fetches and parses a single URL efficiently.\"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Use 'lxml' for C-based speed (pip install lxml)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        \n",
    "        # EFFICIENCY: Target only the core content tags\n",
    "        # Most professional blogs/sites use these tags for their actual content\n",
    "        main_content = soup.find(['article', 'main', 'div.content', 'section'])\n",
    "        if not main_content:\n",
    "            main_content = soup.body # Fallback\n",
    "            \n",
    "        # Clean text: remove script/style/nav\n",
    "        for tag in main_content([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            tag.decompose()\n",
    "            \n",
    "        return main_content.get_text(separator=' ', strip=True)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def batch_scout(url_list):\n",
    "    \"\"\"Scouts multiple competitors in parallel.\"\"\"\n",
    "    print(f\"üöÄ Starting Parallel Scout for {len(url_list)} targets...\")\n",
    "    \n",
    "    # Use 5 threads (don't go too high to avoid getting blocked)\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = list(executor.map(optimized_fetch, url_list))\n",
    "    \n",
    "    for i, content in enumerate(results):\n",
    "        filename = f\"competitor_{i+1}.txt\"\n",
    "        path = os.path.join(INTEL_DIR, filename)\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(content[:4000]) # Limit to 5000 chars for Llama efficiency\n",
    "        print(f\"‚úÖ Saved: {path}\")\n",
    "\n",
    "# Example: Run it in your notebook\n",
    "batch_scout([\"https://openai.com/news\", \"https://www.anthropic.com/news\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c86b19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1354.25it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 17 chunks and saved to /nuvodata/User_data/shiva/Market_carousal/faiss_index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1. Path Configuration\n",
    "FILE_PATH = \"/nuvodata/User_data/shiva/Market_carousal/knowledge.md\"\n",
    "FAISS_SAVE_PATH = \"/nuvodata/User_data/shiva/Market_carousal/faiss_index\"\n",
    "\n",
    "# 2. Markdown Structural Splitting\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "with open(FILE_PATH, 'r') as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = md_splitter.split_text(markdown_content)\n",
    "\n",
    "# 3. Fine-tuning Chunk Size (Optional but recommended)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "final_chunks = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "# 4. Embeddings & FAISS Initialization\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(final_chunks, embeddings)\n",
    "\n",
    "# 5. Save the index locally\n",
    "# This creates a folder containing the index and docstore mapping\n",
    "vector_store.save_local(FAISS_SAVE_PATH)\n",
    "\n",
    "print(f\"FAISS index created with {len(final_chunks)} chunks and saved to {FAISS_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cac9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1056.89it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: * **Company Name:** Nueralogic\n",
      "* **Website:** [https://nueralogic.com](https://nueralogic.com)\n",
      "* **Positioning:** Execution-focused AI solutions and automation partner.\n",
      "* **Core Description:** Nueralogic is an AI solutions company that helps enterprises design, build, and deploy production-grade AI systems that deliver measurable business outcomes. We specialize in custom AI models, intelligent automation, computer vision, LLM-powered systems, and data intelligence platforms.\n",
      "* **The Nueralogic Difference:** Unlike traditional consultants, Nueralogic focuses on execution‚Äîshipping real systems that integrate directly into business workflows. We combine deep technical expertise with cost-efficient, high-tier engineering talent based in India, enabling clients to achieve ~40% cost savings without compromising quality, security, or scalability.  \n",
      "---\n",
      "\n",
      "Result 2: * **Execution over experimentation:** Production-grade AI, not just demos.\n",
      "* **End-to-end ownership:** Strategy, build, deploy, and maintain.\n",
      "* **Cost efficiency:** Elite Indian engineering talent at ~40% lower cost.\n",
      "* **Speed to value:** Rapid deployment with measurable outcomes.\n",
      "* **System integration:** AI embedded directly into existing tools and workflows.  \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load index\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "new_db = FAISS.load_local(\n",
    "    \"/nuvodata/User_data/shiva/Market_carousal/faiss_index\", \n",
    "    embeddings, \n",
    "    allow_dangerous_deserialization=True  # Required for loading local FAISS files\n",
    ")\n",
    "\n",
    "# Search\n",
    "query = \"What is Nueralogic's cost efficiency advantage?\"\n",
    "docs = new_db.similarity_search(query, k=2)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Result {i+1}: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3456e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Chroma System Cache Flushed.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "# This is the \"secret\" command to reset the internal singleton\n",
    "chromadb.api.shared_system_client.SharedSystemClient._identifier_to_system.clear()\n",
    "print(\"üßπ Chroma System Cache Flushed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f8928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ootd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
